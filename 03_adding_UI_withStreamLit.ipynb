{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPRVq3e03c1K"
      },
      "source": [
        "## Chatbot, adding UI with Streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1KvMtf54l0d"
      },
      "source": [
        "### Install App and LLM dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2evPp14fy258",
        "outputId": "cadfd12c-3311-4895-cad7-2b7be2403808"
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.1.12\n",
        "!pip install langchain-openai==0.0.8\n",
        "!pip install langchain-community==0.0.29\n",
        "!pip install streamlit==1.32.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCMshwB1U9iQ"
      },
      "source": [
        "### Write the app code "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXceMDF0Qza0",
        "outputId": "00f2633f-5de2-4fea-97c8-b31f90bbdfd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py \n",
        "#the above line will write as file\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "from operator import itemgetter\n",
        "import streamlit as st\n",
        "import yaml\n",
        "import os\n",
        "import locale\n",
        "\n",
        "\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "with open('api_credentials.yml', 'r') as file:\n",
        "    api_creds = yaml.safe_load(file)\n",
        "\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = api_creds['openai_key']\n",
        "\n",
        "# Customize initial app landing page\n",
        "st.set_page_config(page_title=\"AI Assistant\", page_icon=\"ðŸ¤–\")\n",
        "st.title(\"AI Chat assistant application! \")\n",
        "\n",
        "# Manages live updates to a Streamlit app's display by appending new text tokens\n",
        "# to an existing text stream and rendering the updated text in Markdown\n",
        "class StreamHandler(BaseCallbackHandler):\n",
        "  def __init__(self, container, initial_text=\"\"):\n",
        "    self.container = container\n",
        "    self.text = initial_text\n",
        "\n",
        "  def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
        "    self.text += token\n",
        "    self.container.markdown(self.text)\n",
        "\n",
        "# Load a connection to ChatGPT LLM\n",
        "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.1,streaming=True)\n",
        "\n",
        "# Add a basic system prompt for LLM behavior\n",
        "SYS_PROMPT = \"\"\"\n",
        "              Act as a helpful assistant and answer questions to the best of your ability.\n",
        "              Do not make up new answers.\n",
        "              \"\"\"\n",
        "\n",
        "# Create a prompt template for langchain to use history to answer user prompts\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "  [\n",
        "    (\"system\", SYS_PROMPT),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "  ]\n",
        ")\n",
        "\n",
        "# Create a basic llm chain\n",
        "llm_chain = (\n",
        "  prompt\n",
        "  |\n",
        "  chatgpt\n",
        ")\n",
        "\n",
        "# Store conversation history in Streamlit session state\n",
        "streamlit_msg_history = StreamlitChatMessageHistory()\n",
        "\n",
        "# Create a conversation chain\n",
        "conversation_chain = RunnableWithMessageHistory(\n",
        "  llm_chain,\n",
        "  lambda session_id: streamlit_msg_history,  # Accesses memory\n",
        "  input_messages_key=\"input\",\n",
        "  history_messages_key=\"history\",\n",
        ")\n",
        "\n",
        "# Shows the first message when app starts\n",
        "if len(streamlit_msg_history.messages) == 0:\n",
        "  streamlit_msg_history.add_ai_message(\"How can I help you?\")\n",
        "\n",
        "# Render current messages from StreamlitChatMessageHistory\n",
        "for msg in streamlit_msg_history.messages:\n",
        "  st.chat_message(msg.type).write(msg.content)\n",
        "\n",
        "# If user inputs a new prompt, display it and show the response\n",
        "if user_prompt := st.chat_input():\n",
        "  st.chat_message(\"human\").write(user_prompt)\n",
        "  # This is where response from the LLM is shown\n",
        "  with st.chat_message(\"ai\"):\n",
        "    # Initializing an empty data stream\n",
        "    stream_handler = StreamHandler(st.empty())\n",
        "    config = {\"configurable\": {\"session_id\": \"any\"},\n",
        "              \"callbacks\": [stream_handler]}\n",
        "    # Get llm response\n",
        "    response = conversation_chain.invoke({\"input\": user_prompt},\n",
        "                                         config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8de1tM6FVLsq"
      },
      "source": [
        "## Start the app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Za_TAI2RkPI9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py #--server.port=8989 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
